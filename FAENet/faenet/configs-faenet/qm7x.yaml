# ---- DATASET: QM7-X ----

default:
  frame_averaging: 3D  # {"2D", "3D", "DA", ""}
  fa_method: stochastic  # {"", all, stochastic, det, se3-all, se3-stochastic, se3-det}
  model:
    act: swish
    max_num_neighbors: 40
    preprocess: base_preprocess
    regress_forces: direct_with_gradient_target
    hidden_channels: 500
    num_filters: 400
    num_interactions: 5
    num_gaussians: 50
    cutoff: 5.0
    tag_hidden_channels: 0  # only for OC20
    pg_hidden_channels: 32  # period & group embedding hidden channels
    phys_embeds: True  # physics-aware embeddings for atoms
    phys_hidden_channels: 0
    energy_head: False  # Energy head: {False, weighted-av-initial-embeds, weighted-av-final-embeds}
    skip_co: concat  # Skip connections {False, "add", "concat"}
    second_layer_MLP: True  # in EmbeddingBlock
    complex_mp: True  # 2-layer MLP in Interaction blocks
    mp_type: updownscale_base  # Message Passing type {'base', 'simple', 'updownscale', 'updownscale_base'}
    graph_norm: False  # graph normalization layer
    force_decoder_type: "res_updown" # force head (`"simple"`, `"mlp"`, `"res"`, `"res_updown"`)
    force_decoder_model_config:
      simple:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
      mlp:
        hidden_channels: 256
        norm: batch1d # batch1d, layer or null
      res:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
      res_updown:
        hidden_channels: 128
        norm: batch1d # batch1d, layer or null
  optim:
    batch_size: 100
    eval_batch_size: 100
    ema_decay: 0.999
    fidelity_max_steps: 2000000
    max_steps: 3500000
    scheduler: ReduceLROnPlateau
    optimizer: AdamW
    warmup_steps: 3000
    warmup_factor: 0.2
    threshold: 0.001
    threshold_mode: abs
    lr_initial: 0.000193
    min_lr: 0.000001
    lr_gamma: 0.1
    lr_milestones:
      - 17981
      - 26972
      - 35963
    force_coefficient: 75
    energy_coefficient: 1
    loss_energy: "mae"
    loss_force: "mse"
